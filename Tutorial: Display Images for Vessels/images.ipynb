{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2bd1982",
   "metadata": {},
   "source": [
    "This notebooks requires the following dependencies: `httpx`, `beautifulsoup4`, `followthemoney`.\n",
    "\n",
    "If you are using `pip`, run:\n",
    "`pip install httpx, beautifulsoup4; pip install git+https://github.com/dataresearchcenter/followthemoney.git` \n",
    "\n",
    "If you are  using `poetry`, run:\n",
    "`poetry init; poetry add httpx, beautifulsoup4; poetry add git+https://github.com/dataresearchcenter/followthemoney.git` \n",
    "\n",
    "Run `$(poetry env activate)` to activate the virtual environment that `poetry` creates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179bb3a",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to walk you through a demonstration of scraping Wikipedia for objects along with their respective Wikidata IDs and uploading these to OpenAleph. The result will make use of the feature that allows OpenAleph to display a preview images which it fetches from Wikidata using the Wikidata ID. \n",
    "\n",
    "First, fetch the entire contents of the [Wikipedia list of icebreaker ships](https://en.wikipedia.org/wiki/List_of_icebreakers), per country. \n",
    "\n",
    "Parse this contents using the [BeautifulSoup library](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). Focus on the section of the HTML that contains the list of vessels per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62779fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wiki_dom_request = httpx.get('https://en.wikipedia.org/wiki/List_of_icebreakers')\n",
    "if wiki_dom_request.status_code == 200:\n",
    "    wiki_dom = wiki_dom_request.text\n",
    "\n",
    "wiki_dom_bs = BeautifulSoup(wiki_dom, 'html.parser')\n",
    "content_dom = wiki_dom_bs.find(\"div\", class_=\"mw-content-ltr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807672d",
   "metadata": {},
   "source": [
    "Search for all the `<h2>` and `<ul>` elements. The `<h2>` elements contain the names of countries. The unordered lists (`<ul>`) contain the ships. We skip intermediary classifications of ships. For the purpose of this demo, we only want the icebreaker vessel names and the URLs that point to their Wikipedia articles. We skip the icebreaker ships for which Wikipedia articles don't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c29636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for tag in content_dom.css.select(\"h2, ul\"):\n",
    "    if tag.name == \"h2\":\n",
    "        country = tag.text\n",
    "        if country not in data:\n",
    "            data[country] = []\n",
    "    if tag.name == \"ul\":\n",
    "        for vessel_obj in tag.find_all('li', recursive=False):\n",
    "            link = vessel_obj.find('a', href=True)\n",
    "            if link:\n",
    "                vessel_name = link.get(\"title\")\n",
    "                if not vessel_name or (vessel_name and \"page does not exist\" in vessel_name):\n",
    "                    continue\n",
    "                vessel_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "                data[country].append({\"name\": vessel_name, \"url\": vessel_url})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5dd563",
   "metadata": {},
   "source": [
    "Get the [Wikidata ID](https://en.wikipedia.org/wiki/Wikipedia:Finding_a_Wikidata_ID).\n",
    "\n",
    "There is some finesse here that is skipped for the sake of a simple tutorial. We haven't correctly gotten the ID of all the icebreaker ships due to differnces between the name used by Wikidata ID and the name displayed in the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89ff141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"debug.json\", \"w\") as f:\n",
    "    for country in data:\n",
    "        for vessel_obj in data[country]:\n",
    "            name = vessel_obj[\"name\"].replace(\" \", \"_\")\n",
    "            qid_request = httpx.get(f\"https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles={name}&format=json\")\n",
    "            if qid_request.status_code == 200:\n",
    "                response_json = json.loads(qid_request.text)\n",
    "\n",
    "                f.write(json.dumps(response_json))\n",
    "                f.write(\"\\n\")\n",
    "                                                \n",
    "                pages = response_json[\"query\"][\"pages\"]\n",
    "                for page_id in pages:\n",
    "                    if \"pageprops\" in pages[page_id]:\n",
    "                        qid = pages[page_id][\"pageprops\"].get(\"wikibase_item\")\n",
    "                        if qid:\n",
    "                            vessel_obj[\"qid\"] = qid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1df6db",
   "metadata": {},
   "source": [
    "Create the CSV file which we are going to use in order to create [Follow the Money](https://followthemoney.tech/explorer/) entities using the `Vessel` schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef326aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"icebreakers.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"name\", \"country\", \"qid\" ,\"url\"])\n",
    "    for country in data:\n",
    "        for vessel in data[country]:\n",
    "            writer.writerow([vessel['name'], country, vessel.get('qid') , vessel['url']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
